{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfde8cc6",
   "metadata": {},
   "source": [
    "Intento de crear una CGAN con los datos de minecraft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f313c9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a8159e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "df=pd.read_parquet(\"datos/minecraft.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8add2b9",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb225a8",
   "metadata": {},
   "source": [
    "informacion relevante:\n",
    "las imagenes componen valores entre [-1,1] por eso se usa funcion tanh y no sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dee56ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_imagen(row):\n",
    "    \"\"\"\n",
    "    Muestra una imagen y su etiqueta desde una fila del DataFrame.\n",
    "    \"\"\"\n",
    "    # Obtener los bytes de la imagen\n",
    "    image_bytes = row['image']['bytes']\n",
    "\n",
    "    img = Image.open(io.BytesIO(image_bytes))\n",
    "    \n",
    "    # Mostrar imagen con su etiqueta\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(row['label'])\n",
    "    plt.show()\n",
    "\n",
    "# Dataset único (con augment opcional)\n",
    "class MinecraftDataset(Dataset):\n",
    "    def __init__(self, rows: List[dict], image_size=16, augment=False, transform=None):\n",
    "        self.rows = rows\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((image_size, image_size)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.5]*3, [0.5]*3)\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.rows[idx]\n",
    "        img = Image.open(io.BytesIO(r[\"image\"][\"bytes\"])).convert(\"RGB\")\n",
    "        img_t = self.transform(img)\n",
    "        if self.augment and torch.rand(1).item() > 0.5:\n",
    "            img_t = torch.flip(img_t, dims=[2])\n",
    "        label = r[\"label\"]\n",
    "        return img_t, label\n",
    "\n",
    "\n",
    "# Embedding textual (por palabra)\n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab, embed_dim=100):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.stoi = {w: i for i, w in enumerate(vocab)}\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_vocab(labels):\n",
    "        words = set()\n",
    "        for lbl in labels:\n",
    "            for w in re.findall(r\"\\w+\", lbl.lower()):\n",
    "                words.add(w)\n",
    "        return sorted(words)\n",
    "\n",
    "    def forward(self, label_texts: List[str]):\n",
    "        device = next(self.embedding.parameters()).device\n",
    "        vectors = []\n",
    "        for text in label_texts:\n",
    "            tokens = [w for w in re.findall(r\"\\w+\", text.lower()) if w in self.stoi]\n",
    "            if not tokens:\n",
    "                tokens = [\"unknown\"]\n",
    "            idxs = torch.tensor([self.stoi[w] for w in tokens], device=device)\n",
    "            emb = self.embedding(idxs).mean(dim=0)\n",
    "            vectors.append(emb)\n",
    "        return torch.stack(vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ec5d8",
   "metadata": {},
   "source": [
    "#### Crear Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b71ace37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator (2 capas + salida → 16×16)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=100, embed_dim=100, ngf=32, out_channels=3):\n",
    "        super().__init__()\n",
    "        # proyección inicial a 4×4\n",
    "        self.fc = nn.Linear(nz + embed_dim, ngf * 4 * 4 * 4)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # 4×4 → 8×8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 8×8 → 16×16 (salida)\n",
    "            nn.ConvTranspose2d(ngf * 2, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, cond_emb):\n",
    "        x = torch.cat([z, cond_emb], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), -1, 4, 4)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Discriminator (2 capas + salida)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_dim=100, ndf=32, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.fc_cond = nn.Linear(embed_dim, 16 * 16)  # mapa 1 canal\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # 3+1 = 4 canales → 16×16 → 8×8\n",
    "            nn.Conv2d(in_channels + 1, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 8×8 → 4×4\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # salida (4×4 → escalar)\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(ndf * 4 * 4 * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, cond_emb):\n",
    "        B, C, H, W = img.shape\n",
    "        cond_map = self.fc_cond(cond_emb).view(B, 1, H, W)\n",
    "        x = torch.cat([img, cond_map], dim=1)\n",
    "        return self.net(x).view(-1, 1).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760e717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "nz = 100             # tamaño del vector de ruido\n",
    "embed_dim = 100      # dimensión del embedding textual\n",
    "num_epochs = 30\n",
    "lr = 1e-3 \n",
    "beta1 = 0.5\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0423e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vocabulario generado con 351 palabras únicas:\n"
     ]
    }
   ],
   "source": [
    "# Construir vocabulario\n",
    "all_labels = df[\"label\"].tolist()\n",
    "cleaned_labels = [re.sub(r\"\\d+\", \"\", lbl).strip() for lbl in all_labels]\n",
    "all_labels = list(set(cleaned_labels))\n",
    "\n",
    "vocab = TextEmbedding.build_vocab(all_labels)\n",
    "\n",
    "print(f\" Vocabulario generado con {len(vocab)} palabras únicas:\")\n",
    "# print(vocab)\n",
    "\n",
    "# Dataset y DataLoader\n",
    "dataset = MinecraftDataset(df.to_dict(\"records\"), image_size=16, augment=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec281e4",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELOS\n",
    "txt_emb = TextEmbedding(vocab, embed_dim=embed_dim).to(device)\n",
    "G = Generator(nz=nz, embed_dim=embed_dim, ngf=32).to(device)\n",
    "D = Discriminator(embed_dim=embed_dim, ndf=32).to(device)\n",
    "\n",
    "# Pérdida y optimizadores\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "# comprobar esto\n",
    "\n",
    "# ENTRENAMIENTO\n",
    "for epoch in range(num_epochs):\n",
    "    for real_imgs, labels in dataloader:\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        cond_emb = txt_emb(labels).to(device)\n",
    "        b_size = real_imgs.size(0)\n",
    "\n",
    "        # Etiquetas reales/falsas con label smoothing leve\n",
    "        real_labels = torch.full((b_size,), 0.9, dtype=torch.float, device=device)\n",
    "        fake_labels = torch.full((b_size,), 0.0, dtype=torch.float, device=device)\n",
    "\n",
    "        # =========================================================\n",
    "        # (1) Actualizar Discriminator: maximize log(D(x|y)) + log(1 - D(G(z|y)))\n",
    "        # =========================================================\n",
    "        D.zero_grad()\n",
    "\n",
    "        # Salida del D con imágenes reales\n",
    "        output_real = D(real_imgs, cond_emb)\n",
    "        lossD_real = criterion(output_real, real_labels)\n",
    "\n",
    "        # Generar imágenes falsas condicionales\n",
    "        z = torch.randn(b_size, nz, device=device)\n",
    "        fake_imgs = G(z, cond_emb)\n",
    "        output_fake = D(fake_imgs.detach(), cond_emb)\n",
    "        lossD_fake = criterion(output_fake, fake_labels)\n",
    "\n",
    "        # Pérdida total del Discriminador\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # =========================================================\n",
    "        # (2) Actualizar Generator: maximize log(D(G(z|y)))\n",
    "        # =========================================================\n",
    "        G.zero_grad()\n",
    "        output = D(fake_imgs, cond_emb)\n",
    "        lossG = criterion(output, real_labels)  # quiere que D piense que son reales\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    # =========================================================\n",
    "    # LOGGING\n",
    "    # =========================================================\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Época [{epoch}/{num_epochs}] \"\n",
    "              f\"Loss_D: {lossD.item():.4f}  Loss_G: {lossG.item():.4f}\")\n",
    "\n",
    "        # Generar una imagen de ejemplo condicional\n",
    "        test_prompt = \"oak boat\"\n",
    "        test_emb = txt_emb([test_prompt]).to(device)\n",
    "        z = torch.randn(1, nz, device=device)\n",
    "        fake_test = G(z, test_emb).detach().cpu()\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import torchvision.transforms.functional as TF\n",
    "\n",
    "        img_vis = (fake_test[0] * 0.5 + 0.5).clamp(0, 1)\n",
    "        plt.imshow(TF.to_pil_image(img_vis))\n",
    "        plt.title(f\"Generado para: {test_prompt}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# GUARDAR MODELOS\n",
    "# =========================================================\n",
    "torch.save(G.state_dict(), \"generator_cgan16.pth\")\n",
    "torch.save(D.state_dict(), \"discriminator_cgan16.pth\")\n",
    "print(\"✅ Modelos guardados.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
